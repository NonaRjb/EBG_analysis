wandb: Appending key for api.wandb.ai to your netrc file: /Midgard/home/nonar/.netrc
wandb: Starting wandb agent üïµÔ∏è
2024-02-08 07:51:41,269 - wandb.wandb_agent - INFO - Running runs: []
2024-02-08 07:51:41,597 - wandb.wandb_agent - INFO - Agent received command: run
2024-02-08 07:51:41,597 - wandb.wandb_agent - INFO - Agent starting run with config:
	batch_size: 32
	data: dataset1
	dropout: 0.25
	ebg_transform: tfr_morlet
	eeg: eegnet
	epoch: 300
	hidden_size: 64
	lr: 0.0001
	num_layers: 1
	optim_name: adamw
	seed: 42
	subject_id: 0
	tmax: 0.3
	tmin: -0.1
2024-02-08 07:51:41,609 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python /Midgard/home/nonar/EBG_analysis/train.py --batch_size=32 --data=dataset1 --dropout=0.25 --ebg_transform=tfr_morlet --eeg=eegnet --epoch=300 --hidden_size=64 --lr=0.0001 --num_layers=1 --optim_name=adamw --seed=42 --subject_id=0 --tmax=0.3 --tmin=-0.1
2024-02-08 07:51:46,623 - wandb.wandb_agent - INFO - Running runs: ['jdzgtpgg']
wandb: Currently logged in as: nonar (nona-phd). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /Midgard/home/nonar/EBG_analysis/wandb/run-20240208_075207-jdzgtpgg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/nona-phd/EBG_Olfaction
wandb: üßπ View sweep at https://wandb.ai/nona-phd/EBG_Olfaction/sweeps/xd6sm7gd
wandb: üöÄ View run at https://wandb.ai/nona-phd/EBG_Olfaction/runs/jdzgtpgg
  0%|          | 0/53 [00:00<?, ?it/s]/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:453: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484775609/work/aten/src/ATen/native/Convolution.cpp:883.)
  return F.conv2d(input, weight, bias, self.stride,
  0%|          | 0/53 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/Midgard/home/nonar/EBG_analysis/train.py", line 135, in <module>
    best_model = trainer.train(train_loader, val_loader)
  File "/Midgard/home/nonar/EBG_analysis/models/trainer.py", line 60, in train
    preds = self.model(x)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/EBG_analysis/models/eegnet.py", line 34, in forward
    x = self.model(x)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [8, 1, 1, 64], expected input[1, 32, 4, 1537] to have 1 channels, but got 32 channels instead
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run jumping-sweep-1 at: https://wandb.ai/nona-phd/EBG_Olfaction/runs/jdzgtpgg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240208_075207-jdzgtpgg/logs
Traceback (most recent call last):
  File "/Midgard/home/nonar/EBG_analysis/train.py", line 135, in <module>
    best_model = trainer.train(train_loader, val_loader)
  File "/Midgard/home/nonar/EBG_analysis/models/trainer.py", line 60, in train
    preds = self.model(x)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/EBG_analysis/models/eegnet.py", line 34, in forward
    x = self.model(x)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/Midgard/home/nonar/miniconda3/envs/eegnet_pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [8, 1, 1, 64], expected input[1, 32, 4, 1537] to have 1 channels, but got 32 channels instead
2024-02-08 07:53:12,125 - wandb.wandb_agent - INFO - Cleaning up finished run: jdzgtpgg
2024-02-08 07:53:12,742 - wandb.wandb_agent - INFO - Agent received command: exit
2024-02-08 07:53:12,742 - wandb.wandb_agent - INFO - Received exit command. Killing runs and quitting.
wandb: Terminating and syncing runs. Press ctrl-c to kill.
